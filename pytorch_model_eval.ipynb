{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHNNvKf2JoM5"
   },
   "source": [
    "## Packages and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Puy4lMU-kLt9",
    "outputId": "66b9f896-6839-4100-f1d5-250d9c6e1554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxAHnpZ3kwYw",
    "outputId": "a4aec3ae-b1a6-4714-cff4-f9edfa7da110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 184kB 11.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.2MB 17.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 133kB 46.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 37.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 7.4MB 41.2MB/s \n",
      "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: botocore 1.20.44 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5cq3F1-xwSx",
    "outputId": "82e76d7f-1b4e-4ef5-8d40-8376b82a2e08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 11.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 46.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.10.1 transformers-4.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dBptJvDkl64"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from google.colab import files\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import sleep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from pytorch_transformers import BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction, BertForMultipleChoice, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YDU9ZX-vnzW"
   },
   "source": [
    "## Read in FinBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gsSfyOrr2uZ"
   },
   "outputs": [],
   "source": [
    "FinBERT_classifier = BertForSequenceClassification.from_pretrained('/content/drive/My Drive/W266 Project/FinBERT-Combo_128MSL-100K/', num_labels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ucp1Bg8SoBJb",
    "outputId": "592c57ad-6f0c-49bd-d87b-6959a23897c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pytorch_transformers.modeling_bert.BertForSequenceClassification"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(FinBERT_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkAexA7_p2uN",
    "outputId": "960fd69f-22b5-4ae3-f214-b43a767f1432"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'BertForSequenceClassification(\\n  (bert): BertModel(\\n    (embeddings): BertEmbeddings(\\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\\n      (position_embeddings): Embedding(512, 768)\\n      (token_type_embeddings): Embedding(2, 768)\\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n      (dropout): Dropout(p=0.1, inplace=False)\\n    )\\n    (encoder): BertEncoder(\\n      (layer): ModuleList(\\n        (0): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (1): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (2): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (3): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (4): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (5): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (6): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (7): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (8): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (9): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (10): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (11): BertLayer(\\n          (attention): BertAttention(\\n            (self): BertSelfAttention(\\n              (query): Linear(in_features=768, out_features=768, bias=True)\\n              (key): Linear(in_features=768, out_features=768, bias=True)\\n              (value): Linear(in_features=768, out_features=768, bias=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n            (output): BertSelfOutput(\\n              (dense): Linear(in_features=768, out_features=768, bias=True)\\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n              (dropout): Dropout(p=0.1, inplace=False)\\n            )\\n          )\\n          (intermediate): BertIntermediate(\\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          )\\n          (output): BertOutput(\\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n      )\\n    )\\n    (pooler): BertPooler(\\n      (dense): Linear(in_features=768, out_features=768, bias=True)\\n      (activation): Tanh()\\n    )\\n  )\\n  (dropout): Dropout(p=0.1, inplace=False)\\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\\n)'"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(FinBERT_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqlFF_MUqDds"
   },
   "outputs": [],
   "source": [
    "print(FinBERT_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvMxv1dUqslL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40fNqw8nvrD4"
   },
   "source": [
    "## Read in dataset and create Train, Validation, and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTeakuFStG41"
   },
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('/content/drive/MyDrive/W266 Project/data_augmented_mda_no_numbers_labels_one_day_change.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rsY2BzutG7a",
    "outputId": "391b38d6-35e0-41dc-9d5b-fd51183f22d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data set:\n",
      "\n",
      "(43400, 17)\n",
      "1.0    32538\n",
      "0.0     5471\n",
      "2.0     5391\n",
      "Name: trinary_98_102, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Data set:\", full_data.shape, sep = '\\n')\n",
    "print(full_data.trinary_98_102.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfAxe4vrwDIi"
   },
   "outputs": [],
   "source": [
    "# create training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(full_data.mda_raw_text_no_numbers, full_data.trinary_98_102, test_size=.2)\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyIYX3jjtG95",
    "outputId": "3512db2e-ae08-4392-9c19-4886300f40a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\n",
      "34720\n",
      "1.0    26042\n",
      "0.0     4357\n",
      "2.0     4321\n",
      "Name: trinary_98_102, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\", len(train_labels), sep='\\n\\n')\n",
    "print(train_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhUjeDI0xM5q"
   },
   "outputs": [],
   "source": [
    "# print(\"Validation set:\", len(val_labels), sep='\\n\\n')\n",
    "# print(val_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDHNF4-CtG_-",
    "outputId": "e16524f5-6e71-4920-c4a4-932f6aae75ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set:\n",
      "\n",
      "8680\n",
      "1.0    6496\n",
      "0.0    1114\n",
      "2.0    1070\n",
      "Name: trinary_98_102, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing set:\", len(test_labels), sep='\\n\\n')\n",
    "print(test_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6aJWph7tHB-"
   },
   "outputs": [],
   "source": [
    "## save these so that we can load these in directly without re-randomizing our train and test set\n",
    "\n",
    "# train_texts.to_csv('drive/MyDrive/W266 Project/final_dataset/train_texts.csv', index = False)\n",
    "# train_labels.to_csv('drive/MyDrive/W266 Project/final_dataset/train_labels.csv', index = False)\n",
    "# test_texts.to_csv('drive/MyDrive/W266 Project/final_dataset/test_texts.csv', index = False)\n",
    "# test_labels.to_csv('drive/MyDrive/W266 Project/final_dataset/test_labels.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EDlQUxbxePU"
   },
   "source": [
    "## Tokenize and prepare dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Van_2PS0Bp0d"
   },
   "outputs": [],
   "source": [
    "# train_texts = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/train_texts.csv')\n",
    "# train_labels = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/train_labels.csv')\n",
    "# test_texts = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/test_texts.csv')\n",
    "# test_labels = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1k0_1c9zGvx"
   },
   "outputs": [],
   "source": [
    "# train_texts = train_texts.iloc[:,0]\n",
    "# train_labels = train_labels.iloc[:,0]\n",
    "\n",
    "# train_texts = train_texts.reset_index(drop=True)\n",
    "# train_labels = train_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VMf_A0KVwjC"
   },
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizerFast\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9rfn8umxk6u"
   },
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAuHEV99Pq2M"
   },
   "outputs": [],
   "source": [
    "# print('Train text minimum tokens:',min(map(len,train_encodings['input_ids'])))\n",
    "# print('Train text maximum tokens:',max(map(len,train_encodings['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FBRZh2XmXjf"
   },
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open('/content/drive/MyDrive/W266 Project/final_dataset/train_encodings.pickle', 'rb') as handle:\n",
    "    train_pickle = pickle.load(handle)\n",
    "\n",
    "train_labels = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/train_labels.csv')\n",
    "train_labels = train_labels.loc[:,\"seven_day_binary_100\"]\n",
    "train_labels = train_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhfATE7vso98",
    "outputId": "9892821f-06e3-4f50-cb8e-940103635801"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-j4-XHfEZNi"
   },
   "source": [
    "## Format data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDq8-3Fq1lKv"
   },
   "outputs": [],
   "source": [
    "def truncate_and_pad(encodings):\n",
    "    train_encodings = encodings\n",
    "    \n",
    "    for idx in range(len(train_encodings['input_ids'])):\n",
    "        length_of_tokens = len(train_encodings['input_ids'][idx])\n",
    "        if length_of_tokens <= 512:\n",
    "            num_tokens_to_add = 512 - length_of_tokens\n",
    "            ending_token = train_encodings['input_ids'][idx][-1]\n",
    "            starting_chunk = train_encodings['input_ids'][idx][0:-1]\n",
    "            mid_chunk = [0] * num_tokens_to_add\n",
    "            final_chunk = starting_chunk + mid_chunk + [ending_token]\n",
    "\n",
    "            train_encodings['input_ids'][idx] = final_chunk\n",
    "            train_encodings['token_type_ids'][idx] = train_encodings['token_type_ids'][idx][0:-1] + [0]*num_tokens_to_add + [train_encodings['token_type_ids'][idx][-1]]\n",
    "            train_encodings['attention_mask'][idx] = train_encodings['attention_mask'][idx][0:-1] + [0]*num_tokens_to_add + [train_encodings['attention_mask'][idx][-1]]\n",
    "        else:\n",
    "            midpoint = length_of_tokens // 2\n",
    "            starting_point = midpoint - 255\n",
    "            ending_point = midpoint + 255\n",
    "\n",
    "            mid_chunk = train_encodings['input_ids'][idx][starting_point:ending_point]\n",
    "            starter_token = train_encodings['input_ids'][idx][0]\n",
    "            ending_token = train_encodings['input_ids'][idx][-1]\n",
    "\n",
    "            mid_chunk.insert(0, starter_token)\n",
    "            mid_chunk.append(ending_token)\n",
    "\n",
    "            train_encodings['input_ids'][idx] = mid_chunk\n",
    "            train_encodings['token_type_ids'][idx] = train_encodings['token_type_ids'][idx][0:512]\n",
    "            train_encodings['attention_mask'][idx] = train_encodings['attention_mask'][idx][0:512]\n",
    "    return train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvHEQ-0h_Qym"
   },
   "outputs": [],
   "source": [
    "class TenQDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype = torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def train_model(encodings, labels, num_classes, model_type, epochs, learning_rate, checkpoint_path, use_checkpoint = False):\n",
    "    train_dataset = TenQDataset(encodings, labels)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "\n",
    "    if model_type == 'finbert':\n",
    "        model_string = '/content/drive/My Drive/W266 Project/FinBERT-Combo_128MSL-100K/'\n",
    "    elif model_type == 'bert':\n",
    "        model_string = 'bert-base-uncased'\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(model_string, num_labels = num_classes)\n",
    "    model.to(device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "    optim = AdamW(model.parameters(), lr=learning_rate)\n",
    "    PATH = checkpoint_path\n",
    "\n",
    "    if use_checkpoint:\n",
    "        ## load checkpoint\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        batch_number = checkpoint['batch']\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    num_epochs = epochs\n",
    "    batches_to_checkpoint = 500\n",
    "    for epoch in range(num_epochs):\n",
    "        start_epoch = datetime.datetime.now()\n",
    "        for batch_number, batch in enumerate(train_loader,0):\n",
    "            if batch_number % batches_to_checkpoint == 0:\n",
    "                timestamped_batch = batch_number\n",
    "                starting_loss = 0\n",
    "                starttime = datetime.datetime.now()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            train_loss = loss.item()\n",
    "            starting_loss += train_loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            if batch_number == 1:\n",
    "                print('Starting Batch Loss - {}'.format(train_loss))\n",
    "          \n",
    "            if batch_number == (timestamped_batch + batches_to_checkpoint - 1):\n",
    "                average_loss = starting_loss / batches_to_checkpoint\n",
    "                endtime = datetime.datetime.now()\n",
    "                print('Epoch {}, Batch {}: Average Loss - {}, Most Recent Batch Loss - {}'.format(epoch + 1, batch_number + 1, average_loss, train_loss))\n",
    "                print('\\t\\tPrevious {} batches took'.format(batches_to_checkpoint), endtime - starttime)\n",
    "\n",
    "                print('\\t\\tSaving model to', PATH, end = '\\n\\n')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_number,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optim.state_dict(),\n",
    "                    'loss': average_loss\n",
    "                },  PATH)\n",
    "        end_epoch = datetime.datetime.now()\n",
    "        print('Finished Epoch {}: Final Batch Loss - {}'.format(epoch + 1, train_loss))\n",
    "        print('\\t\\tFull Epoch took', end_epoch - start_epoch)\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': batch_number,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optim.state_dict(),\n",
    "                    'loss': average_loss\n",
    "                },  PATH)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAyjveavEgp0"
   },
   "outputs": [],
   "source": [
    "# ## train from scratch\n",
    "# encodings = chunked_tokenize(train_texts)\n",
    "# processed_data = truncate_and_pad(encodings)\n",
    "# train_model(processed_data, train_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SYsfrr_m9TM"
   },
   "outputs": [],
   "source": [
    "## train from scratch using pickle loaded data\n",
    "# encodings = chunked_tokenize(train_texts)\n",
    "\n",
    "processed_data = truncate_and_pad(train_pickle)\n",
    "train_model(processed_data, \n",
    "            train_labels, \n",
    "            num_classes=2,\n",
    "            model_type = 'finbert',\n",
    "            epochs = 5,\n",
    "            learning_rate = 5e-8, \n",
    "            checkpoint_path = '/content/drive/MyDrive/W266 Project/finbert_model_checkpoint_seven_day_binary_100/model.pt',\n",
    "            use_checkpoint = True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZzA2jyA59Ei"
   },
   "source": [
    "## Load model and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXt4lR_7z5Qz"
   },
   "outputs": [],
   "source": [
    "# full_data = pd.read_csv('/content/drive/MyDrive/W266 Project/data_augmented_mda_no_numbers_labels_one_day_change.csv')\n",
    "\n",
    "# train_texts = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/train_texts.csv')\n",
    "# train_labels = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/train_labels.csv')\n",
    "# test_texts = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/test_texts.csv')\n",
    "# test_labels = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zHeTNW2z5Ot"
   },
   "outputs": [],
   "source": [
    "# test_texts = test_texts.iloc[:,0]\n",
    "# test_labels = test_labels.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOQPidrE6R8G"
   },
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open('/content/drive/MyDrive/W266 Project/final_dataset/test_encodings.pickle', 'rb') as handle:\n",
    "    test_pickle = pickle.load(handle)\n",
    "\n",
    "test_labels = pd.read_csv('/content/drive/MyDrive/W266 Project/final_dataset/test_labels.csv')\n",
    "test_labels = test_labels.loc[:,\"seven_day_binary_100\"]\n",
    "test_labels = test_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oRjFSuMz5MZ"
   },
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atas6BkY0m7W"
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size=int(len(test_dataset.encodings['input_ids']) / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8LM-GxYiF1E"
   },
   "outputs": [],
   "source": [
    "# # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "# print(device)\n",
    "\n",
    "# model_load = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 3)\n",
    "# model_load.to(device)\n",
    "# optim_load = AdamW(model_load.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# PATH = '/content/drive/MyDrive/W266 Project/bert_training_checkpoint/model.pt'\n",
    "# checkpoint = torch.load(PATH, map_location=device)\n",
    "# model_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optim_load.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "# batch_number = checkpoint['batch']\n",
    "\n",
    "# model_load.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUwLcAxn2wf6",
    "outputId": "cd14b91b-48c9-4556-eb61-961e28742ec0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# true_labels = []\n",
    "# predicted_labels = []\n",
    "# for batch_number, batch in enumerate(test_loader,0):\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         true_labels.extend(labels.tolist())\n",
    "\n",
    "#         outputs = model_load(input_ids, attention_mask=attention_mask)\n",
    "#         for row in outputs[0]:\n",
    "#             top = max(row)\n",
    "#             index_of_highest = [i for i,j in enumerate(row) if j == top]\n",
    "#             predicted_labels.extend(index_of_highest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hok3-nu--vv6"
   },
   "outputs": [],
   "source": [
    "# preds_col = pd.DataFrame(predicted_labels, columns = ['preds'])\n",
    "# true_col = pd.DataFrame(true_labels, columns = ['true'])\n",
    "# predictions = pd.concat([true_col, preds_col], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHBe6ER58jq7"
   },
   "outputs": [],
   "source": [
    "def truncate_and_pad(encodings):\n",
    "    train_encodings = encodings\n",
    "    \n",
    "    for idx in range(len(train_encodings['input_ids'])):\n",
    "        length_of_tokens = len(train_encodings['input_ids'][idx])\n",
    "        if length_of_tokens <= 512:\n",
    "            num_tokens_to_add = 512 - length_of_tokens\n",
    "            ending_token = train_encodings['input_ids'][idx][-1]\n",
    "            starting_chunk = train_encodings['input_ids'][idx][0:-1]\n",
    "            mid_chunk = [0] * num_tokens_to_add\n",
    "            final_chunk = starting_chunk + mid_chunk + [ending_token]\n",
    "\n",
    "            train_encodings['input_ids'][idx] = final_chunk\n",
    "            train_encodings['token_type_ids'][idx] = train_encodings['token_type_ids'][idx][0:-1] + [0]*num_tokens_to_add + [train_encodings['token_type_ids'][idx][-1]]\n",
    "            train_encodings['attention_mask'][idx] = train_encodings['attention_mask'][idx][0:-1] + [0]*num_tokens_to_add + [train_encodings['attention_mask'][idx][-1]]\n",
    "        else:\n",
    "            midpoint = length_of_tokens // 2\n",
    "            starting_point = midpoint - 255\n",
    "            ending_point = midpoint + 255\n",
    "\n",
    "            mid_chunk = train_encodings['input_ids'][idx][starting_point:ending_point]\n",
    "            starter_token = train_encodings['input_ids'][idx][0]\n",
    "            ending_token = train_encodings['input_ids'][idx][-1]\n",
    "\n",
    "            mid_chunk.insert(0, starter_token)\n",
    "            mid_chunk.append(ending_token)\n",
    "\n",
    "            train_encodings['input_ids'][idx] = mid_chunk\n",
    "            train_encodings['token_type_ids'][idx] = train_encodings['token_type_ids'][idx][0:512]\n",
    "            train_encodings['attention_mask'][idx] = train_encodings['attention_mask'][idx][0:512]\n",
    "    return train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66LIQqtl-vtu"
   },
   "outputs": [],
   "source": [
    "class TenQDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype = torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def evaluate_model(encodings, labels, num_classes, model_type, checkpoint_path):\n",
    "    test_dataset = TenQDataset(encodings, labels)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "\n",
    "    if model_type == 'finbert':\n",
    "        model_string = '/content/drive/My Drive/W266 Project/FinBERT-Combo_128MSL-100K/'\n",
    "    elif model_type == 'bert':\n",
    "        model_string = 'bert-base-uncased'\n",
    "\n",
    "\n",
    "    model_load = BertForSequenceClassification.from_pretrained(model_string, num_labels = num_classes)\n",
    "    model_load.to(device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "    optim_load = AdamW(model_load.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "    PATH = checkpoint_path\n",
    "    checkpoint = torch.load(PATH, map_location=device)\n",
    "    model_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optim_load.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    batch_number = checkpoint['batch']\n",
    "\n",
    "    model_load.eval()\n",
    "\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for batch_number, batch in enumerate(test_loader,0):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "        outputs = model_load(input_ids, attention_mask=attention_mask)\n",
    "        for row in outputs[0]:\n",
    "            # print(row)\n",
    "            top = max(row)\n",
    "            index_of_highest = [i for i,j in enumerate(row) if j == top]\n",
    "            predicted_labels.extend(index_of_highest)\n",
    "\n",
    "    preds_col = pd.DataFrame(predicted_labels, columns = ['preds'])\n",
    "    true_col = pd.DataFrame(true_labels, columns = ['true'])\n",
    "    predictions = pd.concat([true_col, preds_col], axis = 1)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYTFc095dr-D",
    "outputId": "95f5cbc7-7a98-45cf-aa68-14cdb28da4a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "processed_data = truncate_and_pad(test_pickle)\n",
    "final_preds = evaluate_model(processed_data, \n",
    "                             test_labels, \n",
    "                             num_classes = 2, \n",
    "                             model_type = 'finbert',\n",
    "                             checkpoint_path = '/content/drive/MyDrive/W266 Project/finbert_model_checkpoint_seven_day_binary_100/model.pt'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajMks7b1sV7g",
    "outputId": "c48317ff-ecb8-47dc-8d0a-cb2690ad39eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4531\n",
       "1    4149\n",
       "Name: preds, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.preds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUNPwbessV9h",
    "outputId": "4278e9e2-242f-4f0b-daf9-85255db99128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4356\n",
       "1    4324\n",
       "Name: true, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.true.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNpv78C_sWCD",
    "outputId": "9e319537-c878-4a44-b4da-ad81b413f4d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true  preds\n",
       "0     0        2420\n",
       "      1        1936\n",
       "1     0        2111\n",
       "      1        2213\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.groupby(['true','preds']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCOK87lpOBEE"
   },
   "outputs": [],
   "source": [
    "# final_preds.to_csv('/content/drive/MyDrive/W266 Project/bert_model_checkpoint_seven_day_trinary_98_102/final_preds.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u56xODdzOBIS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPyS5_q5OBKn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mthAgFaKCIm9",
    "outputId": "0b27c80d-492a-426e-c05c-e8706d18f998"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0040, -0.0007, -0.0018, -0.0233, -0.0179,  0.0006,  0.0023,  0.0151,\n",
       "        -0.0107, -0.0182, -0.0090,  0.0017,  0.0172,  0.0077, -0.0047,  0.0115,\n",
       "         0.0234, -0.0387,  0.0039, -0.0092,  0.0335,  0.0046, -0.0217, -0.0158,\n",
       "        -0.0208, -0.0030,  0.0023, -0.0281, -0.0180, -0.0064,  0.0176,  0.0117,\n",
       "        -0.0079, -0.0015,  0.0123,  0.0203,  0.0014, -0.0227, -0.0105,  0.0229,\n",
       "        -0.0114, -0.0200, -0.0075, -0.0540,  0.0136,  0.0091, -0.0027, -0.0096,\n",
       "         0.0053, -0.0206,  0.0248,  0.0141, -0.0011, -0.0179,  0.0010,  0.0078,\n",
       "         0.0005,  0.0081, -0.0332, -0.0122, -0.0358, -0.0067, -0.0179,  0.0225,\n",
       "        -0.0204,  0.0074, -0.0080,  0.0133,  0.0237,  0.0125, -0.0172, -0.0108,\n",
       "        -0.0076, -0.0126, -0.0079, -0.0107, -0.0295,  0.0125,  0.0131, -0.0179,\n",
       "         0.0048, -0.0024, -0.0018,  0.0026,  0.0303,  0.0196, -0.0208, -0.0056,\n",
       "        -0.0059, -0.0292, -0.0004,  0.0072,  0.0153,  0.0049,  0.0286,  0.0027,\n",
       "        -0.0136, -0.0179, -0.0116, -0.0540,  0.0197, -0.0059,  0.0016, -0.0142,\n",
       "         0.0032,  0.0104, -0.0143,  0.0325, -0.0093,  0.0031,  0.0230,  0.0078,\n",
       "         0.0108, -0.0341,  0.0243,  0.0109,  0.0079,  0.0116,  0.0144, -0.0246,\n",
       "         0.0069,  0.0106, -0.0113, -0.0003,  0.0430, -0.0145,  0.0059, -0.0110,\n",
       "         0.0269, -0.0108,  0.0127,  0.0188, -0.0093,  0.0098,  0.0255, -0.0168,\n",
       "         0.0056,  0.0041,  0.0102,  0.0104,  0.0063,  0.0399, -0.0044, -0.0095,\n",
       "         0.0097, -0.0002, -0.0163, -0.0129, -0.0069,  0.0116,  0.0055,  0.0149,\n",
       "        -0.0165,  0.0506,  0.0051, -0.0133,  0.0047, -0.0175,  0.0086,  0.0062,\n",
       "         0.0012, -0.0003,  0.0236,  0.0170, -0.0057,  0.0150, -0.0274,  0.0103,\n",
       "        -0.0027,  0.0054,  0.0063,  0.0079, -0.0164, -0.0031,  0.0302, -0.0339,\n",
       "         0.0065, -0.0115,  0.0071, -0.0197,  0.0054,  0.0004, -0.0196, -0.0186,\n",
       "         0.0101,  0.0003, -0.0284, -0.0143, -0.0115,  0.0254,  0.0090, -0.0100,\n",
       "        -0.0457, -0.0116, -0.0136, -0.0086, -0.0331, -0.0202,  0.0101,  0.0154,\n",
       "        -0.0095,  0.0196,  0.0033, -0.0261,  0.0339, -0.0028, -0.0046,  0.0257,\n",
       "        -0.0209,  0.0060,  0.0192, -0.0055,  0.0174,  0.0093, -0.0032, -0.0046,\n",
       "        -0.0074, -0.0090,  0.0018,  0.0038, -0.0068, -0.0227,  0.0056,  0.0114,\n",
       "         0.0086, -0.0107,  0.0205, -0.0109,  0.0005,  0.0102,  0.0277, -0.0155,\n",
       "        -0.0142, -0.0151, -0.0105, -0.0034, -0.0063, -0.0024, -0.0090,  0.0251,\n",
       "         0.0155, -0.0069, -0.0181,  0.0253, -0.0069,  0.0003, -0.0028, -0.0101,\n",
       "        -0.0088, -0.0034,  0.0275,  0.0132,  0.0361, -0.0246, -0.0080, -0.0255,\n",
       "         0.0111,  0.0228, -0.0045, -0.0214,  0.0060, -0.0094, -0.0001, -0.0131,\n",
       "         0.0166, -0.0066,  0.0259, -0.0013,  0.0151, -0.0004, -0.0086,  0.0026,\n",
       "        -0.0035,  0.0122,  0.0003, -0.0043, -0.0028,  0.0365, -0.0181, -0.0127,\n",
       "         0.0265,  0.0176, -0.0222,  0.0048, -0.0103,  0.0054, -0.0025,  0.0009,\n",
       "        -0.0068,  0.0145,  0.0023,  0.0094, -0.0304, -0.0221,  0.0152,  0.0037,\n",
       "        -0.0249, -0.0086, -0.0156,  0.0125, -0.0033,  0.0114, -0.0086, -0.0040,\n",
       "        -0.0069, -0.0052,  0.0126, -0.0051, -0.0118, -0.0005, -0.0123, -0.0114,\n",
       "         0.0316,  0.0132, -0.0149, -0.0005,  0.0162,  0.0051,  0.0065,  0.0032,\n",
       "         0.0016,  0.0297, -0.0170,  0.0087, -0.0097, -0.0066,  0.0267, -0.0095,\n",
       "         0.0192,  0.0064,  0.0003,  0.0093, -0.0085,  0.0256,  0.0079, -0.0111,\n",
       "        -0.0087, -0.0636, -0.0198,  0.0156, -0.0036,  0.0275, -0.0377, -0.0054,\n",
       "        -0.0054,  0.0176,  0.0193, -0.0262,  0.0095,  0.0037, -0.0054, -0.0233,\n",
       "         0.0156, -0.0072, -0.0025, -0.0026,  0.0102,  0.0097, -0.0009,  0.0034,\n",
       "         0.0028, -0.0254,  0.0227, -0.0007, -0.0137, -0.0264,  0.0129,  0.0092,\n",
       "        -0.0029,  0.0102,  0.0247,  0.0006, -0.0378,  0.0170, -0.0134,  0.0131,\n",
       "         0.0128,  0.0051, -0.0029,  0.0262, -0.0192,  0.0186,  0.0029, -0.0048,\n",
       "         0.0035, -0.0059, -0.0114,  0.0043,  0.0212, -0.0200, -0.0105,  0.0079,\n",
       "         0.0010, -0.0087, -0.0208,  0.0057,  0.0027,  0.0105,  0.0157,  0.0041,\n",
       "         0.0069,  0.0120, -0.0014, -0.0274,  0.0242, -0.0125,  0.0054, -0.0011,\n",
       "        -0.0035, -0.0196, -0.0211, -0.0003, -0.0081, -0.0079,  0.0059, -0.0279,\n",
       "        -0.0147, -0.0198,  0.0054, -0.0109, -0.0033,  0.0100, -0.0109,  0.0213,\n",
       "         0.0005, -0.0063,  0.0134, -0.0204, -0.0133, -0.0131, -0.0075, -0.0021,\n",
       "        -0.0146,  0.0003, -0.0110, -0.0094, -0.0342,  0.0101, -0.0116, -0.0251,\n",
       "        -0.0328, -0.0242, -0.0164, -0.0168,  0.0032, -0.0173, -0.0012,  0.0117,\n",
       "         0.0356,  0.0105, -0.0089, -0.0034, -0.0008,  0.0081, -0.0012, -0.0209,\n",
       "         0.0072, -0.0054, -0.0147,  0.0017, -0.0073,  0.0184,  0.0078, -0.0175,\n",
       "         0.0155, -0.0278,  0.0117,  0.0108, -0.0051, -0.0004, -0.0100,  0.0356,\n",
       "        -0.0015, -0.0234, -0.0097, -0.0189, -0.0027, -0.0254,  0.0110,  0.0147,\n",
       "         0.0076,  0.0027, -0.0023, -0.0163, -0.0002,  0.0113,  0.0183, -0.0058,\n",
       "         0.0195, -0.0296,  0.0006,  0.0045, -0.0018,  0.0073,  0.0037, -0.0137,\n",
       "         0.0144,  0.0143,  0.0050,  0.0153,  0.0021,  0.0005, -0.0210, -0.0017,\n",
       "         0.0039, -0.0018,  0.0139, -0.0022,  0.0082, -0.0002,  0.0060,  0.0018,\n",
       "         0.0184, -0.0057,  0.0013,  0.0136, -0.0059, -0.0022, -0.0013,  0.0159,\n",
       "         0.0069, -0.0002,  0.0238, -0.0144,  0.0021, -0.0010, -0.0239,  0.0033,\n",
       "         0.0021, -0.0229, -0.0074,  0.0430,  0.0079,  0.0085,  0.0033,  0.0167,\n",
       "        -0.0110, -0.0143,  0.0084, -0.0107,  0.0083, -0.0021,  0.0113,  0.0249,\n",
       "         0.0197,  0.0104,  0.0146,  0.0231,  0.0112,  0.0028,  0.0117, -0.0071,\n",
       "        -0.0268, -0.0225,  0.0149, -0.0211,  0.0217, -0.0200,  0.0318,  0.0147,\n",
       "        -0.0012, -0.0046,  0.0161,  0.0022,  0.0156, -0.0047,  0.0126,  0.0167,\n",
       "         0.0138, -0.0004,  0.0080,  0.0319,  0.0131, -0.0047, -0.0091, -0.0042,\n",
       "        -0.0061,  0.0287,  0.0223, -0.0155,  0.0328,  0.0247,  0.0279, -0.0343,\n",
       "        -0.0223,  0.0070,  0.0057,  0.0079,  0.0144, -0.0102, -0.0044, -0.0038,\n",
       "         0.0279,  0.0141, -0.0056, -0.0192,  0.0061,  0.0018,  0.0192,  0.0330,\n",
       "        -0.0189, -0.0042, -0.0148,  0.0288,  0.0066, -0.0033,  0.0039,  0.0062,\n",
       "         0.0096,  0.0111,  0.0025, -0.0033,  0.0032,  0.0072, -0.0064, -0.0026,\n",
       "         0.0005,  0.0031, -0.0046, -0.0169,  0.0169,  0.0057,  0.0028, -0.0022,\n",
       "        -0.0056, -0.0057, -0.0139, -0.0070,  0.0215,  0.0092, -0.0045, -0.0027,\n",
       "         0.0429,  0.0173,  0.0195, -0.0050, -0.0296,  0.0234,  0.0105,  0.0251,\n",
       "         0.0113, -0.0278,  0.0185,  0.0185,  0.0021,  0.0097,  0.0595, -0.0018,\n",
       "        -0.0123, -0.0116,  0.0057,  0.0151,  0.0044,  0.0203, -0.0014, -0.0052,\n",
       "        -0.0307,  0.0166, -0.0214,  0.0314,  0.0202, -0.0366, -0.0026, -0.0146,\n",
       "         0.0127,  0.0255, -0.0120,  0.0067,  0.0014,  0.0221, -0.0089, -0.0084,\n",
       "        -0.0011,  0.0164, -0.0003,  0.0046,  0.0137, -0.0107,  0.0012, -0.0135,\n",
       "         0.0246,  0.0038, -0.0143, -0.0051,  0.0090, -0.0151, -0.0200, -0.0062,\n",
       "         0.0056, -0.0175, -0.0149, -0.0225,  0.0403,  0.0002,  0.0098, -0.0325,\n",
       "         0.0195, -0.0187, -0.0188,  0.0064,  0.0103,  0.0143, -0.0296,  0.0047,\n",
       "         0.0160, -0.0033,  0.0005,  0.0269, -0.0070,  0.0100, -0.0079, -0.0016,\n",
       "         0.0104, -0.0076,  0.0112,  0.0043,  0.0180,  0.0088, -0.0269, -0.0123,\n",
       "         0.0022,  0.0136, -0.0147,  0.0120, -0.0096, -0.0134,  0.0110,  0.0069,\n",
       "        -0.0051, -0.0177,  0.0040,  0.0391,  0.0059, -0.0159,  0.0094, -0.0339,\n",
       "        -0.0191,  0.0199, -0.0078, -0.0123,  0.0128,  0.0348,  0.0053, -0.0241,\n",
       "         0.0053,  0.0336,  0.0325, -0.0009, -0.0131,  0.0141, -0.0092,  0.0136,\n",
       "        -0.0032,  0.0209, -0.0238, -0.0023,  0.0081, -0.0018, -0.0107,  0.0174,\n",
       "         0.0082,  0.0187,  0.0091, -0.0449, -0.0048, -0.0184,  0.0368,  0.0083],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load.state_dict()['classifier.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNLUo3i1B2mN",
    "outputId": "512b7bbd-dd6d-41cd-b807-581c086d4057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4482e-02, -6.9611e-05,  3.3875e-02,  1.7152e-02, -1.9951e-02,\n",
       "         4.0673e-02,  7.3961e-03,  1.8944e-02, -1.9960e-02,  4.1581e-02,\n",
       "         3.1574e-02, -5.9357e-03,  8.6846e-03, -2.9385e-02, -8.0777e-03,\n",
       "         8.3111e-03,  9.0202e-03, -8.2728e-03, -1.8894e-02, -1.0812e-02,\n",
       "         1.7028e-02,  3.4429e-03, -1.9861e-02, -4.1194e-03,  2.8645e-02,\n",
       "        -7.3001e-03, -1.7193e-02, -1.2476e-02,  1.1415e-02,  5.8531e-04,\n",
       "        -3.0735e-02,  3.5565e-02, -1.4902e-02,  2.9362e-02, -1.2896e-02,\n",
       "        -3.5882e-03, -1.4492e-02, -2.0595e-02,  1.6306e-02,  1.5280e-02,\n",
       "        -4.8266e-03, -1.7178e-02, -1.8658e-02,  1.3059e-02,  4.0989e-03,\n",
       "         1.0589e-02, -1.2625e-02,  1.8632e-02,  2.8249e-02, -4.1579e-02,\n",
       "         3.9141e-03, -2.4041e-02,  4.0459e-02,  8.0854e-03, -1.9295e-02,\n",
       "        -3.4466e-02,  6.0362e-03,  1.4995e-02, -3.8931e-02, -1.4457e-02,\n",
       "         9.1001e-03,  5.3663e-05,  7.6696e-03,  1.2852e-02, -3.7079e-02,\n",
       "         9.3357e-03,  2.4936e-02,  9.6161e-03, -1.1563e-03, -3.1547e-02,\n",
       "        -6.2061e-02, -1.3988e-02,  2.2828e-02,  4.4027e-02,  4.6285e-02,\n",
       "        -7.8171e-03, -3.3505e-02,  3.7708e-02,  4.6853e-03, -2.1890e-03,\n",
       "         3.4256e-02, -3.7311e-02, -3.0098e-02,  1.0779e-02,  3.5782e-02,\n",
       "        -2.4061e-02, -4.3223e-02,  1.8102e-02, -5.9268e-03,  1.3557e-02,\n",
       "         3.4615e-02, -5.7973e-04,  1.9634e-02,  1.1058e-02, -1.0304e-03,\n",
       "         1.5959e-02,  1.2433e-02, -1.9456e-02,  6.3175e-03, -5.9099e-05,\n",
       "        -3.2372e-02, -3.9804e-03,  1.3929e-02, -3.9189e-03, -1.8935e-02,\n",
       "         3.8586e-02,  1.7186e-02, -2.4350e-02,  1.4912e-02, -1.4542e-02,\n",
       "         2.3354e-02, -2.5436e-02, -1.5559e-02, -7.2682e-03, -2.7269e-02,\n",
       "         9.0478e-04,  1.2078e-02, -1.6171e-02, -1.1663e-04, -1.4931e-02,\n",
       "         1.1934e-02,  1.0809e-02,  1.6693e-02,  4.7255e-03,  2.6545e-02,\n",
       "         1.1809e-02,  1.2226e-02,  7.2023e-03,  4.9842e-03, -3.0810e-02,\n",
       "        -1.4585e-03,  1.1585e-02, -3.1529e-03,  1.8429e-02,  9.7042e-03,\n",
       "         3.3343e-02, -2.1620e-02,  2.0261e-02,  3.9240e-04, -5.6626e-03,\n",
       "        -2.0912e-02, -1.0455e-02,  2.2437e-02, -1.1288e-02,  1.7642e-02,\n",
       "         9.9626e-04, -4.4198e-02,  3.0969e-02, -2.6570e-02, -9.9528e-03,\n",
       "         2.8542e-02, -3.5561e-04, -1.7019e-02,  7.7671e-03,  2.8286e-02,\n",
       "        -1.6654e-03, -1.4735e-02,  1.5715e-03, -3.0774e-02, -1.0584e-02,\n",
       "         6.3594e-02, -5.9669e-03,  1.1674e-02,  1.8685e-02, -1.7968e-03,\n",
       "         2.4904e-02,  8.9464e-04, -1.7017e-03,  7.3560e-03,  1.2022e-02,\n",
       "        -2.7395e-02,  1.3497e-03, -1.0288e-02, -1.4875e-02,  1.2663e-02,\n",
       "         2.2290e-02, -2.8552e-02, -1.8256e-03, -6.6643e-03,  6.4189e-04,\n",
       "         1.5331e-02, -9.6409e-03,  2.7855e-02,  1.3115e-02, -2.6021e-02,\n",
       "        -3.1880e-02,  1.2728e-02,  2.7297e-02,  4.9579e-03,  4.3445e-03,\n",
       "        -4.0457e-03, -5.3696e-03, -2.0049e-02,  1.0685e-02,  1.8740e-03,\n",
       "        -1.0421e-02,  4.1455e-02, -8.3113e-03,  9.4232e-03,  6.9913e-04,\n",
       "         1.5487e-02, -1.8566e-02, -1.0261e-02,  6.3277e-03,  8.1526e-03,\n",
       "        -4.2314e-03,  1.1705e-02,  1.7354e-02, -2.3301e-02, -3.8652e-02,\n",
       "         7.0956e-03, -2.8788e-03, -6.1526e-02,  7.3488e-03, -1.9286e-02,\n",
       "         1.4041e-03,  8.4494e-03,  2.3083e-02,  4.6481e-02,  1.4551e-02,\n",
       "        -3.9633e-02, -3.2268e-02,  1.2439e-02,  1.0177e-02, -2.1401e-02,\n",
       "        -2.3735e-03, -1.0339e-02, -2.6134e-03,  3.2163e-03,  6.9224e-03,\n",
       "         2.3688e-02,  3.5039e-02, -1.7704e-03,  6.0326e-05,  1.0249e-02,\n",
       "         1.7044e-02,  2.3851e-02, -2.1818e-02,  2.8069e-02, -1.2279e-02,\n",
       "        -4.2959e-03, -5.2675e-03,  1.7455e-02, -2.2684e-02,  1.1535e-02,\n",
       "        -2.7165e-03,  1.2452e-03,  2.2243e-02,  2.5399e-02, -3.7960e-02,\n",
       "         4.1031e-02, -1.1173e-02,  6.5312e-03, -5.7100e-03, -1.0111e-02,\n",
       "        -3.6379e-04, -2.6796e-02, -9.7630e-03,  2.2810e-02, -3.6241e-02,\n",
       "         1.3808e-02, -2.9619e-03,  1.2363e-02,  8.4778e-03,  3.4024e-02,\n",
       "         8.2106e-03,  2.4110e-02, -2.5600e-02, -1.9918e-02,  1.9553e-02,\n",
       "         7.8942e-03, -1.7113e-02,  2.8921e-02,  1.1281e-02,  2.3311e-02,\n",
       "        -8.5028e-03,  5.2609e-03,  6.3066e-03,  1.8718e-02,  4.3295e-03,\n",
       "         6.6904e-03,  1.5939e-02,  2.3703e-03,  5.1676e-02, -1.8309e-02,\n",
       "         4.0106e-02, -2.4049e-02,  2.0625e-02, -1.0042e-02,  4.2095e-02,\n",
       "         4.6679e-02,  1.2474e-02, -9.0427e-03, -3.0068e-02,  1.4113e-02,\n",
       "        -3.6428e-03,  1.0324e-02,  2.6814e-03,  7.8092e-03,  3.1447e-02,\n",
       "        -1.3234e-02, -4.7801e-03, -5.7813e-03,  2.7616e-02,  6.9000e-03,\n",
       "         8.5194e-04,  8.5251e-03, -3.1297e-03, -9.3270e-03, -7.7200e-03,\n",
       "         7.7589e-03,  7.6153e-04, -1.4718e-02, -3.1733e-03,  9.0839e-03,\n",
       "         8.7231e-03,  3.3729e-03,  4.2034e-03, -2.9396e-02, -1.5315e-02,\n",
       "         9.5992e-03,  2.1105e-03, -8.4857e-03,  1.6253e-02,  7.0516e-03,\n",
       "         1.9971e-02, -1.9895e-02, -4.2158e-03,  1.2861e-02,  1.0974e-02,\n",
       "        -2.2056e-03, -3.4442e-02,  9.7432e-03,  2.1345e-02,  1.8284e-02,\n",
       "         4.8856e-02, -1.0702e-02,  1.5460e-02,  3.9092e-02,  2.8861e-02,\n",
       "         1.2698e-02,  2.2266e-02, -2.3831e-02, -1.7707e-02,  1.4267e-02,\n",
       "        -2.5777e-02, -6.1273e-03,  8.5363e-03,  5.9883e-02,  2.2048e-02,\n",
       "        -7.5805e-03, -2.2903e-02,  3.7966e-03,  2.9894e-03,  3.3096e-02,\n",
       "        -1.5468e-03, -2.3633e-02, -1.2600e-02,  6.8355e-03,  1.3612e-02,\n",
       "         2.7500e-02,  1.2401e-03, -3.4805e-02,  2.9384e-02,  1.0444e-02,\n",
       "        -1.8561e-02,  4.4526e-02,  1.5725e-02, -1.4511e-02, -1.0661e-02,\n",
       "        -1.8987e-02,  1.1016e-02,  1.3917e-02, -4.0581e-02,  6.6231e-03,\n",
       "        -5.1096e-02, -2.6015e-02,  4.2689e-04,  7.1302e-03,  3.7879e-03,\n",
       "         3.2436e-02, -1.4112e-02,  9.3127e-03, -6.9639e-03,  3.2260e-02,\n",
       "         5.3435e-04, -2.5358e-02,  2.4311e-02,  5.0674e-04,  1.2368e-02,\n",
       "        -6.2952e-03, -1.7509e-02,  3.1514e-02,  3.4268e-02, -1.6128e-02,\n",
       "         5.2244e-03,  1.0635e-02,  1.4931e-02, -1.5623e-02, -1.9461e-02,\n",
       "         9.4321e-03, -2.2715e-02, -3.8951e-02,  9.0188e-03,  1.8283e-02,\n",
       "        -2.3717e-02, -1.1419e-03, -5.2545e-02, -2.6943e-02, -3.5594e-03,\n",
       "         1.1205e-02,  3.2956e-02, -1.3808e-03, -3.8000e-02, -1.2506e-02,\n",
       "        -3.5069e-03, -2.0443e-02, -2.2564e-02, -4.2099e-02,  1.2599e-02,\n",
       "         1.7363e-02,  9.9910e-03, -9.7778e-04,  2.2081e-03,  1.4317e-02,\n",
       "        -1.8339e-02,  2.0784e-02,  1.7108e-02,  1.3463e-02, -1.6686e-02,\n",
       "        -1.5546e-02,  4.5585e-04,  1.2456e-02,  2.0358e-03,  3.3748e-03,\n",
       "         2.4821e-02, -1.5599e-02, -3.0970e-02, -5.3769e-03,  1.3392e-03,\n",
       "         2.3941e-02, -1.1175e-02,  4.5185e-03,  7.5039e-03, -7.3588e-03,\n",
       "         1.2445e-02,  1.4800e-02, -7.2331e-03,  2.4805e-03, -4.6923e-03,\n",
       "        -2.7347e-02, -1.4598e-03, -1.5125e-02, -3.3071e-02,  1.9131e-02,\n",
       "         9.9856e-03, -1.8233e-03,  3.2883e-02, -1.0683e-02, -3.0615e-02,\n",
       "        -3.3653e-02, -6.1690e-03, -5.4645e-03, -1.1698e-04, -1.7293e-03,\n",
       "         1.9127e-02,  1.0102e-02, -1.5685e-02, -2.0417e-02, -7.4114e-03,\n",
       "        -8.4301e-03,  2.0701e-02, -1.1113e-02,  2.7263e-02, -2.5087e-02,\n",
       "         8.5751e-03,  2.0963e-02, -2.4747e-02,  4.7598e-02, -3.5475e-02,\n",
       "         3.1387e-02,  2.8817e-02, -8.1281e-03,  2.5093e-03,  3.4776e-03,\n",
       "         4.9292e-03, -7.0210e-03,  1.3462e-02, -1.7990e-03,  1.6753e-02,\n",
       "         3.2919e-03, -1.5479e-02, -7.8750e-03,  1.2793e-02, -1.9881e-02,\n",
       "         6.6476e-03, -4.0192e-03, -3.3846e-05, -3.8195e-02,  1.7522e-03,\n",
       "        -4.7669e-03,  1.8270e-02, -1.0508e-02,  5.1891e-03, -2.3157e-02,\n",
       "        -1.5542e-02,  3.2315e-02, -6.1903e-03,  2.9223e-02,  2.6850e-02,\n",
       "        -2.3415e-02,  5.4677e-04, -5.7962e-02, -5.0199e-02, -1.7440e-03,\n",
       "         1.1406e-02,  2.7881e-03, -8.9968e-03, -3.0056e-02, -1.0341e-02,\n",
       "        -2.4348e-02,  3.3574e-03,  3.6829e-03, -9.1003e-03,  3.0241e-03,\n",
       "        -9.4855e-03, -2.1051e-02,  1.3631e-03, -1.2089e-02, -4.1349e-03,\n",
       "         2.1463e-03,  2.8093e-03,  4.6806e-02,  1.2735e-02, -1.6774e-02,\n",
       "         2.0533e-02,  2.6959e-02, -6.9439e-03, -1.7206e-02,  4.7397e-02,\n",
       "        -1.0606e-02, -4.9012e-03,  5.7950e-02, -2.0926e-02, -1.6549e-02,\n",
       "         1.2778e-02, -3.5437e-02, -1.6898e-02, -3.2583e-02,  3.9931e-03,\n",
       "         9.5312e-03,  1.1281e-02, -1.7133e-02, -1.8136e-02,  2.3292e-02,\n",
       "        -6.8177e-03,  6.6309e-03,  1.9863e-02,  2.0265e-02,  7.4193e-03,\n",
       "        -3.8241e-02, -1.1878e-02,  3.1540e-03, -2.3352e-02,  9.6955e-03,\n",
       "         3.2640e-02, -2.5089e-03,  8.2322e-04,  1.5779e-02, -1.0677e-02,\n",
       "        -7.4160e-04,  1.9067e-02, -2.4237e-02, -1.6489e-02,  1.6736e-02,\n",
       "         7.2610e-03, -8.8219e-03, -8.1456e-03,  1.6000e-02,  9.0022e-03,\n",
       "         1.7179e-02,  1.3994e-02,  9.0832e-04, -9.9810e-03, -2.0989e-03,\n",
       "        -9.0891e-04,  3.1634e-02, -2.1247e-02, -3.9657e-02,  1.4560e-02,\n",
       "         2.9636e-02,  1.5645e-02, -5.7990e-03,  3.0114e-02,  1.3156e-02,\n",
       "         9.1589e-03,  2.3789e-04, -2.1667e-02, -1.3832e-02,  6.9438e-03,\n",
       "         1.5006e-03, -1.1603e-02,  3.4866e-02, -3.3372e-02,  2.5227e-02,\n",
       "        -1.2765e-02,  1.2144e-03,  5.9676e-03, -6.8407e-03, -1.3717e-02,\n",
       "        -8.0812e-03,  1.0278e-02, -2.2266e-02,  9.3311e-03, -4.0207e-02,\n",
       "         1.1561e-04,  2.1533e-02, -1.9103e-02,  1.5324e-02, -3.9533e-02,\n",
       "         1.1063e-02, -6.1599e-03,  2.1247e-02, -5.8979e-03, -3.6825e-03,\n",
       "        -4.8647e-03, -6.1277e-03, -2.3109e-02, -2.4346e-02,  5.7507e-03,\n",
       "        -1.0003e-02,  2.0316e-03, -2.6464e-02, -1.9851e-03, -7.9594e-03,\n",
       "        -1.7465e-02,  1.6371e-02, -1.7133e-02, -2.2274e-02, -9.4304e-03,\n",
       "        -7.4864e-03,  1.5974e-02,  9.6867e-03,  1.6755e-02, -3.3083e-02,\n",
       "        -1.2133e-02, -1.9679e-02,  3.8375e-02,  9.8116e-03, -2.5998e-03,\n",
       "         2.4018e-02,  2.2197e-02,  3.4691e-03, -2.2549e-03,  1.9749e-02,\n",
       "         1.9419e-03, -1.5260e-03,  3.4895e-02,  2.6395e-03,  1.7313e-02,\n",
       "        -1.3437e-02,  4.3337e-03,  3.0287e-02,  1.5963e-02, -9.4431e-04,\n",
       "        -2.7438e-03, -1.3764e-02,  2.6215e-02, -9.6563e-04, -2.8297e-02,\n",
       "         8.3265e-03, -8.5040e-04, -1.3355e-02,  1.9686e-03, -5.1342e-02,\n",
       "        -1.1163e-03, -1.2771e-02, -2.9353e-02, -1.4822e-02,  6.4111e-04,\n",
       "         9.8070e-04,  1.7604e-02,  1.1707e-02,  2.1640e-03, -5.1675e-02,\n",
       "        -2.3582e-02, -2.2295e-02,  1.2755e-02,  1.5525e-03, -6.4496e-03,\n",
       "        -8.5511e-03, -1.1528e-02, -4.2064e-03, -2.3647e-02,  3.0823e-02,\n",
       "        -2.4939e-02,  3.2999e-02, -2.7164e-03, -7.0399e-02,  3.7615e-03,\n",
       "        -2.8436e-02, -1.7913e-02,  6.4807e-03, -1.1423e-04,  5.6344e-04,\n",
       "        -3.5737e-03, -2.0269e-02, -8.0332e-03, -2.8819e-02,  1.0806e-02,\n",
       "        -4.5173e-03,  2.7778e-03,  1.3245e-02,  3.8374e-03, -4.0450e-03,\n",
       "        -2.3291e-02,  2.2650e-02,  1.8754e-02,  2.5613e-02, -6.5697e-03,\n",
       "        -1.0959e-02, -8.0330e-03,  5.9460e-03, -9.0146e-03,  2.3334e-03,\n",
       "        -3.2349e-03,  1.3492e-02, -2.0619e-02, -1.5937e-02,  1.3771e-02,\n",
       "        -2.6657e-02,  4.9324e-03,  8.2422e-03,  2.7479e-04, -2.9147e-02,\n",
       "        -2.4390e-03, -3.7494e-03,  9.9646e-03, -3.5739e-02, -4.0016e-02,\n",
       "         1.4026e-03, -3.9036e-02, -2.1854e-02, -1.5767e-03,  3.6959e-02,\n",
       "        -1.5631e-02, -1.1571e-02,  2.9580e-02, -2.5195e-02,  1.9029e-03,\n",
       "        -1.6643e-02,  1.0988e-02,  3.6873e-03, -7.6927e-03, -2.2736e-02,\n",
       "         7.5904e-03, -8.4645e-03, -2.3581e-02,  4.6709e-02, -4.0484e-03,\n",
       "         7.3763e-03,  3.4500e-03,  3.5848e-02, -7.5669e-03,  6.9832e-04,\n",
       "         8.2351e-03, -1.1633e-02,  6.1670e-03], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['classifier.weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQMOGX5EBvPp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Data Modeling 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
